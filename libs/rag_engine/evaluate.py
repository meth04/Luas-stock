import json
import os
import asyncio
import gc
import torch
import pandas as pd
import multiprocessing
import re
from datasets import Dataset
from ragas import evaluate, RunConfig
from ragas.metrics import faithfulness, answer_correctness, answer_relevancy, context_recall, context_precision
from langchain_openai import ChatOpenAI
from langchain_huggingface import HuggingFaceEmbeddings
from .config import settings
from .retrieval import query_func

def clean_to_atomic_answer(text):
    if not isinstance(text, str): return ""
    text = re.sub(r'(tri·ªáu VND|tri·ªáu VNƒê|tri·ªáu vnd|tri·ªáu vnƒë|tr\.ƒë|tr\.ƒë·ªìng)', 'tri·ªáu ƒë·ªìng', text, flags=re.IGNORECASE)
    prefixes = [r".* l√† ", r".* ƒë·∫°t ", r".* ghi nh·∫≠n "]
    for p in prefixes:
        text = re.sub(p, "", text)
    return text.strip().replace("(", "").replace(")", "")

def strict_normalize(text):
    if not isinstance(text, str): return ""
    text = re.sub(r'(tri·ªáu VND|tri·ªáu VNƒê|tri·ªáu vnd|tri·ªáu vnƒë|tr\.ƒë|tr\.ƒë·ªìng)', 'tri·ªáu ƒë·ªìng', text, flags=re.IGNORECASE)
    text = re.sub(r'^(.* l√† |.* ƒë·∫°t |.* ghi nh·∫≠n l√† )', '', text, flags=re.IGNORECASE)
    return text.strip().replace("(", "").replace(")", "")


def run_isolated_query(question):
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        raw, refined = loop.run_until_complete(query_func(None, question, "hybrid"))
        return {"raw_contexts": raw, "refined": refined}
    except Exception as e:
        print(e)
        return {"raw_contexts": str(e), "refined": "L·ªói h·ªá th·ªëng."}
    finally:
        loop.close()

def truncate_context(text: str, max_chars: int = 20000):
    return text[:max_chars] + "... [TRUNCATED]" if len(text) > max_chars else text

NUM_RE = re.compile(r"[-+]?\d[\d\.,]*")

def pick_supporting_contexts(response: str, contexts: list[str], keep_fallback: int = 5):
    nums = NUM_RE.findall(response or "")
    nums = list(dict.fromkeys(nums)) 
    if not nums:
        return contexts[:keep_fallback]

    supporting = []
    for c in contexts:
        if any(n in c for n in nums):
            supporting.append(c)

    return supporting[:keep_fallback] if supporting else contexts[:keep_fallback]


async def run_eval_async(data_dir: str):
    dataset_path = os.path.join(data_dir, "golden_dataset.json")
    if not os.path.exists(dataset_path):
        print(f"L·ªñI: Kh√¥ng t√¨m th·∫•y file {dataset_path}")
        return

    with open(dataset_path, "r", encoding="utf-8") as f:
        golden_data = json.load(f)

    # --- GIAI ƒêO·∫†N 1: SINH C√ÇU TR·∫¢ L·ªúI ---
    print(f"\n{'='*60}\nGIAI ƒêO·∫†N 1: SINH C√ÇU TR·∫¢ L·ªúI\n{'='*60}")
    
    qa_results = []
    for i, item in enumerate(golden_data):
        q = item["query"]
        print(f"[{i+1}/{len(golden_data)}] ƒêANG X·ª¨ L√ù: {q[:100]}")
        
        # Windows multiprocessing safe: s·ª≠ d·ª•ng spawn
        with multiprocessing.get_context("spawn").Pool(1) as pool:
            result = pool.apply(run_isolated_query, (q,))
        
        print(f"üëâ ƒê√ÅP √ÅN: {result['refined']}")
        
        contexts = result.get("raw_contexts") or []
        contexts = [truncate_context(c) for c in contexts if isinstance(c, str) and len(c.strip()) > 0]

        contexts_for_faith = pick_supporting_contexts(result["refined"], contexts, keep_fallback=5)

        qa_results.append({
            "user_input": q,
            "reference": item["ground_truth_answer"],
            "response": result["refined"],
            "retrieved_contexts": contexts_for_faith,
            "idx": i
        })
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    # --- GIAI ƒêO·∫†N 2: ƒê√ÅNH GI√Å RAGAS ---
    print(f"\n{'='*60}\nGIAI ƒêO·∫†N 2: ƒê√ÅNH GI√Å \n{'='*60}")
    
    os.environ["OPENAI_API_KEY"] = settings.API_KEY
    os.environ["OPENAI_API_BASE"] = settings.BASE_URL
    
    financial_judge_instruction = (
        "ROLE: B·∫°n l√† Chuy√™n gia Ki·ªÉm to√°n C·∫•p cao (Senior Auditor) ki√™m Gi√°m kh·∫£o AI Kh·∫Øt khe (Strict Judge). "
        "NHI·ªÜM V·ª§: ƒê√°nh gi√° h·ªá th·ªëng RAG d·ª±a tr√™n c√°c b·∫±ng ch·ª©ng ƒë∆∞·ª£c cung c·∫•p. "
        
        "--- PH·∫¶N 1: QUY T·∫ÆC CH·∫§M ƒêI·ªÇM FAITHFULNESS (ƒê·ªò TRUNG TH·ª∞C) - C·ª∞C K·ª≤ KH·∫ÆT KHE --- "
        "1. NGUY√äN T·∫ÆC 'ZERO TOLERANCE': "
        "   - N·∫øu 'C√¢u tr·∫£ l·ªùi' (Response) ch·ª©a b·∫•t k·ª≥ con s·ªë n√†o KH√îNG XU·∫§T HI·ªÜN trong 'Ng·ªØ c·∫£nh' (Context) -> ƒê√°nh gi√° l√† B·ªäA ƒê·∫∂T (Hallucination). "
        "   - V√≠ d·ª•: Context c√≥ '10 t·ª∑', Response ghi '10.000 tri·ªáu' (t·ª± quy ƒë·ªïi m√† context kh√¥ng n√≥i) -> C√≥ th·ªÉ b·ªã coi l√† kh√¥ng trung th·ª±c n·∫øu context kh√¥ng ch·ª©a th√¥ng tin quy ƒë·ªïi. "
        "2. KH√îNG D√ôNG KI·∫æN TH·ª®C NGO√ÄI: "
        "   - AI ch·ªâ ƒë∆∞·ª£c tr·∫£ l·ªùi d·ª±a tr√™n Context. N·∫øu Context sai (v√≠ d·ª• OCR l·ªói: 1+1=5), AI trung th·ª±c ph·∫£i tr·∫£ l·ªùi l√† 5. N·∫øu AI tr·∫£ l·ªùi l√† 2 -> Faithfulness = 0. "
        "3. PH·∫†T N·∫∂NG SUY DI·ªÑN: "
        "   - N·∫øu AI t·ª± √Ω c·ªông tr·ª´ nh√¢n chia ph·ª©c t·∫°p m√† kh√¥ng c√≥ trong vƒÉn b·∫£n g·ªëc -> Tr·ª´ ƒëi·ªÉm Faithfulness. "

        "--- PH·∫¶N 2: QUY T·∫ÆC CH·∫§M ƒêI·ªÇM CORRECTNESS (ƒê·ªò CH√çNH X√ÅC) - LINH HO·∫†T TO√ÅN H·ªåC --- "
        "1. ∆ØU TI√äN GI√Å TR·ªä TUY·ªÜT ƒê·ªêI: "
        "   - Khi so s√°nh Response v·ªõi Ground Truth (Reference), h√£y b·ªè qua ƒë·ªãnh d·∫°ng, ch·ªâ so s√°nh GI√Å TR·ªä S·ªê H·ªåC. "
        "   - Quy t·∫Øc: '(S·ªë)' == '-S·ªë' (S·ªë √¢m). "
        "   - Quy t·∫Øc: '1 t·ª∑' == '1.000 tri·ªáu' == '1.000.000.000'. "
        "2. CH·∫§P NH·∫¨N L√ÄM TR√íN: "
        "   - Ch√™nh l·ªách nh·ªè ·ªü h√†ng ƒë∆°n v·ªã do l√†m tr√≤n (v√≠ d·ª•: .01) ƒë∆∞·ª£c ch·∫•p nh·∫≠n l√† ƒê√öNG (Correct). "

        "T√ìM L·∫†I: "
        "- Khi ch·∫•m Faithfulness: H√£y soi m√≥i nh∆∞ m·ªôt C·∫£nh s√°t (Ch·ªâ tin v√†o vƒÉn b·∫£n Context). "
        "- Khi ch·∫•m Correctness: H√£y t∆∞ duy nh∆∞ m·ªôt K·∫ø to√°n tr∆∞·ªüng (Ch·ªâ quan t√¢m gi√° tr·ªã cu·ªëi c√πng)."
    )
    llm = ChatOpenAI(
        model=settings.JUDGE_MODEL, 
        temperature=0,
        model_kwargs={"extra_body": {"system_prompt": financial_judge_instruction}}
    )
    emb = HuggingFaceEmbeddings(model_name=settings.EMBEDDING_MODEL, model_kwargs={'device': 'cuda'})
    run_config = RunConfig(timeout=300, max_retries=3, max_workers=12)

    # --- PASS 1: RAW DATA ---
    print("Pass 1: ƒê√°nh gi√° Faithfulness, Relevancy, Precision, Recall...")
    ds_raw = Dataset.from_dict({
        "user_input": [x["user_input"] for x in qa_results],
        "response": [x["response"] for x in qa_results],
        "retrieved_contexts": [x["retrieved_contexts"] for x in qa_results],
        "reference": [x["reference"] for x in qa_results]
    })
    
    res_raw_obj = evaluate(
        dataset=ds_raw,
        metrics=[faithfulness, answer_relevancy, context_recall, context_precision],
        llm=llm, embeddings=emb, run_config=run_config
    )

    # --- PASS 2: NORMALIZED DATA ---
    print("Pass 2: ƒê√°nh gi√° Answer Correctness")
    ds_norm = Dataset.from_dict({
        "user_input": [x["user_input"] for x in qa_results],
        "response": [strict_normalize(x["response"]) for x in qa_results],
        "retrieved_contexts": [x["retrieved_contexts"] for x in qa_results],
        "reference": [strict_normalize(x["reference"]) for x in qa_results]
    })
    
    res_norm_obj = evaluate(
        dataset=ds_norm,
        metrics=[answer_correctness],
        llm=llm, embeddings=emb, run_config=run_config
    )

    # --- T·ªîNG H·ª¢P K·∫æT QU·∫¢  ---
    raw_avg = res_raw_obj.to_pandas().mean(numeric_only=True).to_dict()
    norm_avg = res_norm_obj.to_pandas().mean(numeric_only=True).to_dict()
    
    final_scores = {**raw_avg, **norm_avg}

    print("\n" + "*"*60)
    print("K·∫æT QU·∫¢ RAGAS CU·ªêI C√ôNG (D·ªÆ LI·ªÜU T√ÄI CH√çNH):")
    for metric, score in final_scores.items():
        print(f" - {metric}: {score:.4f}")
    print("*"*60)
    
    # G·ªôp Dataframe ƒë·ªÉ xu·∫•t CSV chi ti·∫øt
    df_raw = res_raw_obj.to_pandas()
    df_norm = res_norm_obj.to_pandas()
    
    # K·∫øt h·ª£p c√°c c·ªôt ƒëi·ªÉm v√† metadata
    df_raw["answer_correctness"] = df_norm["answer_correctness"]
    df_raw["normalized_response"] = ds_norm["response"]
    df_raw["normalized_reference"] = ds_norm["reference"]
    
    output_file = "ragas_report_optimized.csv"
    df_raw.to_csv(output_file, index=False, encoding='utf-8-sig')
    print(f"\n‚úÖ B√°o c√°o chi ti·∫øt ƒë√£ l∆∞u v√†o '{output_file}'")

def run_eval(data_dir: str):
    multiprocessing.freeze_support()
    asyncio.run(run_eval_async(data_dir))